{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVErPUmr7tFN"
      },
      "source": [
        "# ShakespeareBot: Transformer-based Shakespearean QA System\n",
        "\n",
        "This notebook implements a Shakespeare-style question answering system using both traditional NLP techniques (Markov chains) and modern transformer models (BERT + GPT). We'll compare both approaches and evaluate their performance."
      ],
      "id": "TVErPUmr7tFN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FSiknA7tFP"
      },
      "source": [
        "## Part 1: Setup and Dependencies"
      ],
      "id": "_4FSiknA7tFP"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7kH1uSy7tFP",
        "outputId": "f4e5096e-50f2-4f84-bcff-4dff32a66c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18606 sha256=23bcd06155202f7658de7aafcbaab12e9a5a4e781cae8f18c34f7263cd800e0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/20/eb/1a3fb93f3132f2f9683e4efd834800f80c53aeddf50e84ae80\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.4.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install markovify\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "id": "W7kH1uSy7tFP"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlYr7c1T7tFQ",
        "outputId": "dffe59dd-0538-4d10-cbfc-661c8156fc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "import re\n",
        "import markovify\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg  # consists of all shakespeare novels\n",
        "import warnings\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer, get_scheduler\n",
        "# Import AdamW from torch.optim instead of transformers\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('gutenberg')"
      ],
      "id": "JlYr7c1T7tFQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mO4r0Bz7tFR"
      },
      "source": [
        "## Part 2: Traditional Approach - Markov Chain Model\n",
        "### 2.1 Data Preparation and Cleaning"
      ],
      "id": "4mO4r0Bz7tFR"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5SSq2ka7tFR",
        "outputId": "14085be2-e721-414d-ec4e-de3a93304ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw: [The Tragedie of Hamlet by William Shakespeare 1599]\n",
            "\n",
            "\n",
            "Actus Primus. Scoena Prima.\n",
            "\n",
            "Enter Barnardo and Francisco two Centinels.\n",
            "\n",
            "  Barnardo. Who's there?\n",
            "  Fran. Nay answer me: Stand & vnfold\n",
            "your sel\n",
            "Raw: [The Tragedie of Macbeth by William Shakespeare 1603]\n",
            "\n",
            "\n",
            "Actus Primus. Scoena Prima.\n",
            "\n",
            "Thunder and Lightning. Enter three Witches.\n",
            "\n",
            "  1. When shall we three meet againe?\n",
            "In Thunder, Lightning, or in Rai\n",
            "Raw: [The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
            "\n",
            "\n",
            "Actus Primus. Scoena Prima.\n",
            "\n",
            "Enter Flauius, Murellus, and certaine Commoners ouer the Stage.\n",
            "\n",
            "  Flauius. Hence: home you idle Creatures, g\n"
          ]
        }
      ],
      "source": [
        "# Import novels as text objects\n",
        "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
        "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
        "\n",
        "# Print first 200 characters of each\n",
        "print('Raw:', hamlet[:200])\n",
        "print('Raw:', macbeth[:200])\n",
        "print('Raw:', caesar[:200])"
      ],
      "id": "E5SSq2ka7tFR"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gZ-Oa_eR7tFR"
      },
      "outputs": [],
      "source": [
        "# Text cleaning function\n",
        "def text_cleaner(text):\n",
        "    text = re.sub(r'--', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'(\\\\b|\\\\s+\\\\-?|^\\\\-?)(\\\\d+|\\\\d*\\\\.\\\\d+)', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Remove chapter indicator\n",
        "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
        "macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
        "caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
        "\n",
        "# Apply cleaning function to corpus\n",
        "hamlet = text_cleaner(hamlet)\n",
        "caesar = text_cleaner(caesar)\n",
        "macbeth = text_cleaner(macbeth)"
      ],
      "id": "gZ-Oa_eR7tFR"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX1_2vSI7tFS",
        "outputId": "33734e41-632e-4753-99ff-5d50cf9d0c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actus Primus. Scoena Prima. Enter Barnardo and Francisco two Centinels. Barnardo. Who's there? Fran. Nay answer me: Stand & vnfold your selfe Bar. Long liue the King Fran. Barnardo? Bar. He Fran. You come most carefully vpon your houre Bar. 'Tis now strook twelue, get thee to bed Francisco Fran. For this releefe much thankes: 'Tis bitter cold, And I am sicke at heart Barn. Haue you had quiet Guard? Fran. Not a Mouse stirring Barn. Well, goodnight. If you do meet Horatio and Marcellus, the Riuals\n"
          ]
        }
      ],
      "source": [
        "# Parse cleaned novels\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "hamlet_doc = nlp(hamlet)\n",
        "macbeth_doc = nlp(macbeth)\n",
        "caesar_doc = nlp(caesar)\n",
        "\n",
        "hamlet_sents = ' '.join([sent.text for sent in hamlet_doc.sents if len(sent.text) > 1])\n",
        "macbeth_sents = ' '.join([sent.text for sent in macbeth_doc.sents if len(sent.text) > 1])\n",
        "caesar_sents = ' '.join([sent.text for sent in caesar_doc.sents if len(sent.text) > 1])\n",
        "\n",
        "# Combine all Shakespeare text\n",
        "shakespeare_sents = hamlet_sents + macbeth_sents + caesar_sents\n",
        "\n",
        "# Print a sample of our processed text\n",
        "print(shakespeare_sents[:500])"
      ],
      "id": "JX1_2vSI7tFS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elt40iRl7tFS"
      },
      "source": [
        "### 2.2 Building the Markov Chain Model"
      ],
      "id": "elt40iRl7tFS"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7zZZHaF7tFS",
        "outputId": "41b55eed-6a35-4e8b-a0db-f8d0cdf82213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Markov model outputs:\n",
            "None\n",
            "None\n",
            "Yes, bring me word Luc.\n"
          ]
        }
      ],
      "source": [
        "# Create text generator using markovify\n",
        "generator_1 = markovify.Text(shakespeare_sents, state_size=3)\n",
        "\n",
        "# Randomly generate three sentences\n",
        "print(\"Basic Markov model outputs:\")\n",
        "for i in range(3):\n",
        "    print(generator_1.make_sentence())"
      ],
      "id": "t7zZZHaF7tFS"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIHAJfmf7tFT",
        "outputId": "79793334-0eed-477f-8f91-0fed35ca940c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short sentences:\n",
            "Once more goodnight, And when you do them- Brut.\n",
            "Hye you Messala, And I will bring him to the Capitoll Por.\n",
            "My Lord, do as you please, But if you would driue me into a toyle?\n"
          ]
        }
      ],
      "source": [
        "# Randomly generate three more sentences of no more than 100 characters\n",
        "print(\"Short sentences:\")\n",
        "for i in range(3):\n",
        "    print(generator_1.make_short_sentence(max_chars=100))"
      ],
      "id": "eIHAJfmf7tFT"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I986daw7tFT",
        "outputId": "54bab0b1-1714-43e6-a524-5210528b31f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS-enhanced Markov model outputs:\n",
            "Vpon my Head they plac'd a fruitlesse Crowne , And put it in his Pocket Qu .\n",
            "Set on , and it marres him ; it sets him on , and not mine owne .\n",
            "Being thus benetted round with Villaines , Ere I had euer seene that day Horatio .\n",
            "Romans , Countrey - men , and it marres him ; it sets him on , and leaue you so .\n",
            "Go Captaine , from me greet the Danish King , Tell him his prankes haue been too broad to beare with me , That I must be idle .\n"
          ]
        }
      ],
      "source": [
        "# Use spacy's part of speech to generate more legible text\n",
        "class POSifiedText(markovify.Text):\n",
        "    def word_split(self, sentence):\n",
        "        return ['::'.join((word.orth_, word.pos_)) for word in nlp(sentence)]\n",
        "    def word_join(self, words):\n",
        "        sentence = ' '.join(word.split('::')[0] for word in words)\n",
        "        return sentence\n",
        "\n",
        "# Call the class on our text\n",
        "generator_2 = POSifiedText(shakespeare_sents, state_size=3)\n",
        "\n",
        "# Now we will use the above generator to generate sentences\n",
        "print(\"POS-enhanced Markov model outputs:\")\n",
        "for i in range(5):\n",
        "    print(generator_2.make_sentence())"
      ],
      "id": "_I986daw7tFT"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhGxxlqK7tFT",
        "outputId": "d4c5ea7b-307f-442d-c729-7eaf603a5b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short POS-enhanced sentences:\n",
            "Masters , you are a Gentleman .\n",
            "Enter Macbeth , Lenox , Soldiers .\n",
            "Thy Master is a Wise and Valiant Romane , I neuer gaue you ought Ophe .\n",
            "It is not madnesse That I haue longed long to re - deliuer .\n",
            "Poore Birds they are not Ham .\n"
          ]
        }
      ],
      "source": [
        "# Print 100 characters or less sentences\n",
        "print(\"Short POS-enhanced sentences:\")\n",
        "for i in range(5):\n",
        "    print(generator_2.make_short_sentence(max_chars=100))"
      ],
      "id": "ZhGxxlqK7tFT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HapQNtfY7tFT"
      },
      "source": [
        "### 2.3 Simple Question-Answering with Markov Model"
      ],
      "id": "HapQNtfY7tFT"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTRwJ9U97tFT",
        "outputId": "84b41611-45be-4a13-d8cc-0db9573cce07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Markov Chain Q&A Demo:\n",
            "------------------------------\n",
            "Q: What is love?\n",
            "A: I rather tell thee what is to be buried in't ?\n",
            "\n",
            "Q: How do I know if I'm in love?\n",
            "A: To be, or not to be, that is the question.\n",
            "\n",
            "Q: What is the meaning of life and how should we live it?\n",
            "A: In what particular thought to work , I know not , Sir Brut .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def markov_answer_question(question, model=generator_2):\n",
        "    \"\"\"\n",
        "    A simple function to generate a Shakespeare-style response\n",
        "    to a question using the Markov chain model\n",
        "    \"\"\"\n",
        "    # Basic logic: longer questions get longer answers\n",
        "    words = len(question.split())\n",
        "\n",
        "    if words <= 3:\n",
        "        return model.make_short_sentence(max_chars=80) or \"Brevity is the soul of wit.\"\n",
        "    elif words <= 6:\n",
        "        return model.make_short_sentence(max_chars=120) or \"The quality of mercy is not strained.\"\n",
        "    else:\n",
        "        return model.make_sentence() or \"To be, or not to be, that is the question.\"\n",
        "\n",
        "# Test with some sample questions\n",
        "sample_questions = [\n",
        "    \"What is love?\",\n",
        "    \"How do I know if I'm in love?\",\n",
        "    \"What is the meaning of life and how should we live it?\"\n",
        "]\n",
        "\n",
        "print(\"Markov Chain Q&A Demo:\")\n",
        "print(\"-\" * 30)\n",
        "for question in sample_questions:\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {markov_answer_question(question)}\")\n",
        "    print()"
      ],
      "id": "mTRwJ9U97tFT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Z4laHn7tFT"
      },
      "source": [
        "## Part 3: Transformer-Based Approach\n",
        "### 3.1 Preparing Shakespeare Data for Fine-Tuning"
      ],
      "id": "z1Z4laHn7tFT"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV5rQJs67tFT",
        "outputId": "bd833eb4-758f-408a-cad5-cea3cae137c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training sentences: 2405\n",
            "Example sentences:\n",
            "[1] Enter Barnardo and Francisco two Centinels.\n",
            "[2] Nay answer me: Stand & vnfold your selfe Bar.\n",
            "[3] You come most carefully vpon your houre Bar.\n",
            "[4] 'Tis now strook twelue, get thee to bed Francisco Fran.\n",
            "[5] For this releefe much thankes: 'Tis bitter cold, And I am sicke at heart Barn.\n"
          ]
        }
      ],
      "source": [
        "# Extract sentences from Shakespeare's works for training\n",
        "def extract_training_sentences(doc, min_length=40, max_length=200):\n",
        "    \"\"\"Extract sentences suitable for training from a spaCy doc\"\"\"\n",
        "    sentences = []\n",
        "    for sent in doc.sents:\n",
        "        text = sent.text.strip()\n",
        "        # Filter for sentences of appropriate length and complexity\n",
        "        if min_length <= len(text) <= max_length and len(text.split()) >= 5:\n",
        "            sentences.append(text)\n",
        "    return sentences\n",
        "\n",
        "# Extract training sentences from each play\n",
        "hamlet_training = extract_training_sentences(hamlet_doc)\n",
        "macbeth_training = extract_training_sentences(macbeth_doc)\n",
        "caesar_training = extract_training_sentences(caesar_doc)\n",
        "\n",
        "# Combine all training sentences\n",
        "shakespeare_training = hamlet_training + macbeth_training + caesar_training\n",
        "\n",
        "# Print some statistics and examples\n",
        "print(f\"Total training sentences: {len(shakespeare_training)}\")\n",
        "print(\"Example sentences:\")\n",
        "for i in range(5):\n",
        "    print(f\"[{i+1}] {shakespeare_training[i]}\")"
      ],
      "id": "DV5rQJs67tFT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPNhSGGJ7tFU"
      },
      "source": [
        "### 3.2 Building the BERT Question Encoder"
      ],
      "id": "SPNhSGGJ7tFU"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Sx-WCYb07tFU"
      },
      "outputs": [],
      "source": [
        "class ShakespeareBertEncoder:\n",
        "    \"\"\"Uses BERT to encode questions for the Shakespeare QA system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def encode_question(self, question):\n",
        "        \"\"\"Encode a question to get its embedding representation\"\"\"\n",
        "        inputs = self.tokenizer(question, return_tensors=\"pt\",\n",
        "                               padding=True, truncation=True, max_length=64)\n",
        "\n",
        "        # Move inputs to device\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Get the [CLS] token embedding (question representation)\n",
        "        question_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return question_embedding\n",
        "\n",
        "    def get_relevant_sentence_ids(self, question, sentences, top_k=5):\n",
        "        \"\"\"Find the most relevant sentences to a question based on embedding similarity\"\"\"\n",
        "        question_emb = self.encode_question(question)\n",
        "\n",
        "        # Encode all sentences\n",
        "        sentence_embeddings = []\n",
        "        for sentence in tqdm(sentences, desc=\"Encoding sentences\", leave=False):\n",
        "            with torch.no_grad():\n",
        "                inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model(**inputs)\n",
        "                sentence_emb = outputs.last_hidden_state[:, 0, :]\n",
        "                sentence_embeddings.append(sentence_emb)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for sent_emb in sentence_embeddings:\n",
        "            similarity = torch.cosine_similarity(question_emb, sent_emb)\n",
        "            similarities.append(similarity.item())\n",
        "\n",
        "        # Get top-k indices\n",
        "        top_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "        return top_indices, [similarities[i] for i in top_indices]"
      ],
      "id": "Sx-WCYb07tFU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk_1s91j7tFU"
      },
      "source": [
        "### 3.3 GPT-2 Shakespeare Response Generator"
      ],
      "id": "kk_1s91j7tFU"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFK-8TBk7tFU",
        "outputId": "ddd46b78-c402-476d-a8f0-d6797f726068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Q&A pairs for training:\n",
            "Question: Can you explain fate?\n",
            "Answer: Cassius, Be not deceiu'd: If I haue veyl'd my looke, I turne the trouble of my Countenance Meerely vpon my selfe.\n",
            "Question: How should I think about jealousy?\n",
            "Answer: O worthyest Cousin, The sinne of my Ingratitude euen now Was heauie on me.\n",
            "Question: What would you say about loyalty?\n",
            "Answer: Throw Physicke to the Dogs, Ile none of it.\n"
          ]
        }
      ],
      "source": [
        "class ShakespeareGPTGenerator:\n",
        "    \"\"\"Generates Shakespeare-style responses using a fine-tuned GPT-2 model\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Add special tokens for Q&A format\n",
        "        special_tokens = {'pad_token': '<PAD>', 'sep_token': '<SEP>'}\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def prepare_training_data(self, qa_pairs):\n",
        "        \"\"\"Prepare input data for fine-tuning\"\"\"\n",
        "        inputs = []\n",
        "        for question, answer in qa_pairs:\n",
        "            # Format: \"Question: {question} <SEP> Answer: {answer}\"\n",
        "            text = f\"Question: {question} {self.tokenizer.sep_token} Answer: {answer}\"\n",
        "            inputs.append(text)\n",
        "        return inputs\n",
        "\n",
        "    def fine_tune(self, training_data, epochs=3, batch_size=4):\n",
        "        \"\"\"Fine-tune the GPT-2 model on Shakespeare data\"\"\"\n",
        "        # Prepare the dataset\n",
        "        encoded_inputs = self.tokenizer(training_data, padding=True, truncation=True,\n",
        "                                       max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "            encoded_inputs[\"input_ids\"],\n",
        "            encoded_inputs[\"attention_mask\"]\n",
        "        )\n",
        "\n",
        "        # Create data loader\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Setup optimizer and scheduler\n",
        "        optimizer = AdamW(self.model.parameters(), lr=5e-5)\n",
        "        scheduler = get_scheduler(\n",
        "                      name=\"linear\",\n",
        "                      optimizer=optimizer,\n",
        "                      num_warmup_steps=0,\n",
        "                      num_training_steps=len(dataloader) * epochs)\n",
        "\n",
        "        # Training loop\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch in tqdm(dataloader, desc=f\"Training epoch {epoch+1}\"):\n",
        "                batch = [item.to(self.device) for item in batch]\n",
        "                input_ids, attention_mask = batch\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Update parameters\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(dataloader)\n",
        "            print(f\"Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "        print(\"Fine-tuning complete!\")\n",
        "        # Save the model\n",
        "        self.model.save_pretrained('./models/shakespeare_gpt')\n",
        "        self.tokenizer.save_pretrained('./models/shakespeare_gpt')\n",
        "        print(\"Model saved to ./models/shakespeare_gpt\")\n",
        "\n",
        "    def generate_response(self, question, max_length=150):\n",
        "        \"\"\"Generate a Shakespeare-style response to a question\"\"\"\n",
        "        # Format the input with the question\n",
        "        prompt = f\"Question: {question} {self.tokenizer.sep_token} Answer:\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generate response\n",
        "        output_sequences = self.model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the response\n",
        "        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract just the answer part\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "# Helper function to convert Shakespeare text to Q&A pairs for training\n",
        "def create_qa_pairs(sentences, num_pairs=500):\n",
        "    \"\"\"Create synthetic Q&A pairs from Shakespeare sentences for training\"\"\"\n",
        "    # Common Shakespeare-related questions\n",
        "    question_templates = [\n",
        "        \"What is {topic}?\",\n",
        "        \"How does one find {topic}?\",\n",
        "        \"Why is {topic} important?\",\n",
        "        \"What would you say about {topic}?\",\n",
        "        \"How should I think about {topic}?\",\n",
        "        \"What advice can you give about {topic}?\",\n",
        "        \"What does it mean to experience {topic}?\",\n",
        "        \"How would you describe {topic}?\",\n",
        "        \"What is the nature of {topic}?\",\n",
        "        \"Can you explain {topic}?\"\n",
        "    ]\n",
        "\n",
        "    # Common Shakespeare themes\n",
        "    topics = [\n",
        "        \"love\", \"death\", \"honor\", \"ambition\", \"jealousy\", \"revenge\", \"power\",\n",
        "        \"betrayal\", \"loyalty\", \"fate\", \"time\", \"truth\", \"deception\", \"madness\",\n",
        "        \"grief\", \"friendship\", \"courage\", \"fear\", \"wisdom\", \"folly\"\n",
        "    ]\n",
        "\n",
        "    qa_pairs = []\n",
        "    for _ in range(num_pairs):\n",
        "        topic = random.choice(topics)\n",
        "        question = random.choice(question_templates).format(topic=topic)\n",
        "        answer = random.choice(sentences)\n",
        "        qa_pairs.append((question, answer))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "# Example usage of the training data generation\n",
        "qa_pairs = create_qa_pairs(shakespeare_training, num_pairs=100)\n",
        "print(\"Sample Q&A pairs for training:\")\n",
        "for i in range(3):\n",
        "    print(f\"Question: {qa_pairs[i][0]}\")\n",
        "    print(f\"Answer: {qa_pairs[i][1]}\")\n",
        "\n",
        "\n",
        "class ShakespeareQASystem:\n",
        "    \"\"\"Complete Shakespeare QA system that combines both approaches\"\"\"\n",
        "\n",
        "    def __init__(self, use_transformer=True, train_model=False):\n",
        "        \"\"\"Initialize the QA system\n",
        "\n",
        "        Args:\n",
        "            use_transformer: Whether to use transformer models (True) or Markov chains (False)\n",
        "            train_model: Whether to train the GPT model from scratch\n",
        "        \"\"\"\n",
        "        self.use_transformer = use_transformer\n",
        "\n",
        "        # Initialize Markov model for comparison or fallback\n",
        "        self.markov_model = generator_2  # Using our POS-enhanced model\n",
        "\n",
        "        # Initialize transformer models if requested\n",
        "        if use_transformer:\n",
        "            print(\"Initializing transformer models...\")\n",
        "            self.bert_encoder = ShakespeareBertEncoder()\n",
        "            self.gpt_generator = ShakespeareGPTGenerator()\n",
        "\n",
        "            # Train the model if requested\n",
        "            if train_model:\n",
        "                print(\"Preparing to fine-tune GPT-2 on Shakespeare texts...\")\n",
        "                # Create training data\n",
        "                qa_pairs = create_qa_pairs(shakespeare_training, num_pairs=500)\n",
        "                training_data = self.gpt_generator.prepare_training_data(qa_pairs)\n",
        "\n",
        "                # Fine-tune the model\n",
        "                self.gpt_generator.fine_tune(training_data, epochs=2, batch_size=2)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Answer a question using either Markov chains or transformers\"\"\"\n",
        "        if not self.use_transformer:\n",
        "            # Use the Markov model approach\n",
        "            return markov_answer_question(question, self.markov_model)\n",
        "        else:\n",
        "            # Use the transformer pipeline\n",
        "            try:\n",
        "                return self.gpt_generator.generate_response(question)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with transformer model: {e}\")\n",
        "                # Fallback to Markov model\n",
        "                return markov_answer_question(question, self.markov_model)\n",
        "\n",
        "    def interactive_chat(self):\n",
        "        \"\"\"Run an interactive chat session with the system\"\"\"\n",
        "        print(\"=== Shakespeare Question-Answering System ===\")\n",
        "        print(f\"Using: {'Transformer models' if self.use_transformer else 'Markov chains'}\")\n",
        "        print(\"Ask a question or type 'exit' to quit.\")\n",
        "\n",
        "        while True:\n",
        "            question = input(\"Your question: \")\n",
        "            if question.lower() in ['exit', 'quit', 'bye']:\n",
        "                print(\"Farewell, good night, parting is such sweet sorrow.\")\n",
        "                break\n",
        "\n",
        "            response = self.answer_question(question)\n",
        "            print(f\"Shakespeare: {response}\")"
      ],
      "id": "kFK-8TBk7tFU"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e29b205c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29b205c",
        "outputId": "358a8d77-5ed5-4d05-c30c-9027d7470dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Shakespeare Question-Answering System ===\n",
            "Using: Markov chains\n",
            "Ask a question or type 'exit' to quit.\n",
            "Your question: what is love?\n",
            "Shakespeare: I can not tell  but I shame To weare a Kerchiefe ?\n",
            "Your question: how to make memories?\n",
            "Shakespeare: This is a sorry sight Macb .\n",
            "Your question: exit\n",
            "Farewell, good night, parting is such sweet sorrow.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Use simpler Markov model for quick testing (no need to load transformers)\n",
        "    qa_system = ShakespeareQASystem(use_transformer=False, train_model=False)\n",
        "\n",
        "    # Run interactive chat session\n",
        "    qa_system.interactive_chat()\n",
        "    # qa_system = ShakespeareQASystem(use_transformer=True, train_model=True)\n",
        "    # qa_system.interactive_chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Use simpler Markov model for quick testing (no need to load transformers)\n",
        "    qa_system = ShakespeareQASystem(use_transformer=True, train_model=False)\n",
        "\n",
        "    # Run interactive chat session\n",
        "    qa_system.interactive_chat()\n",
        "    # qa_system = ShakespeareQASystem(use_transformer=True, train_model=True)\n",
        "    # qa_system.interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHsGr8B2AB-Y",
        "outputId": "fbb59194-b6db-412c-adc9-f7402dda08ea"
      },
      "id": "UHsGr8B2AB-Y",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing transformer models...\n",
            "=== Shakespeare Question-Answering System ===\n",
            "Using: Transformer models\n",
            "Ask a question or type 'exit' to quit.\n",
            "Your question: What is love?\n",
            "Shakespeare: Love is a thing of great love and a thing of great need. For it is not a love which is wholly necessary to you; it is a desire of your own self, which is one which must be cherished by all. It is a desire of your own self which is one which must be cherished by all. Love is the desire of your own self and will not be forgotten, for it will be cherished in the same manner as love. Love is the desire of your own self and will not be forgotten, for it will be cherished in the same manner as love. The desire of your own self is one of one which must be cherished by all. It is a desire of your own\n",
            "Your question: how to make memories ?\n",
            "Shakespeare: You should use these as a starting point for any memory retrieval, and as such are best used in the context of a study that deals with remembering objects. The best practice is to use your own memory as a starting point, with the help of a group of friends. Some people like to make a new memory of their first encounter with the world, and others use a small collection of memories to re-create their previous ones. To do this, first use the group memory as your starting point, and then follow the instructions of the group. A group memory is simply a set of memories of a particular point in time, and is not the same as a complete picture of the world. If\n",
            "Your question: exit\n",
            "Farewell, good night, parting is such sweet sorrow.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}